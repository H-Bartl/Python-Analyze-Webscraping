{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38eaa22b-ba0d-4263-9b32-f0efb1393f27",
   "metadata": {},
   "source": [
    "# Heatmap\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Reset font to default\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
    "\n",
    "# Folder with JSON files\n",
    "folder_path = '/Users/hamzabartl/Documents/DataSciencePy/pythonProject/7528718/german-tweet-sample-2019-04'\n",
    "\n",
    "# Reading all JSON files\n",
    "all_tweets = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.json.gz'):\n",
    "        with gzip.open(os.path.join(folder_path, filename), 'rt', encoding='utf-8', errors='ignore') as file:\n",
    "            try:\n",
    "                json_data = json.load(file)\n",
    "                all_tweets.extend(json_data)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Fehler beim Lesen der Datei: {filename}\")\n",
    "\n",
    "# Converting JSON data to DataFrame\n",
    "df = pd.json_normalize(all_tweets, sep='_')\n",
    "\n",
    "# Ensure the 'text' column is of type string and remove NaNs in 'text'\n",
    "df['text'] = df['text'].astype(str)\n",
    "df = df.dropna(subset=['text'])\n",
    "\n",
    "# Remove rts\n",
    "df = df[~df['text'].str.startswith('RT')]\n",
    "\n",
    "# Ensure the 'created_at' column is treated as datetime\n",
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "\n",
    "# Extract hour and weekday\n",
    "df['hour'] = df['created_at'].dt.hour\n",
    "df['weekday'] = df['created_at'].dt.dayofweek\n",
    "\n",
    "# Aggregate data by hour and weekday\n",
    "heatmap_data = df.groupby(['weekday', 'hour']).size().unstack(fill_value=0)\n",
    "\n",
    "# Creating the heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(heatmap_data, cmap='YlGnBu', linewidths=.5)\n",
    "plt.xlabel('Stunde des Tages')\n",
    "plt.ylabel('Wochentag')\n",
    "plt.title('Twitter-Aktivität zu verschiedenen Tageszeiten und Wochentagen')\n",
    "plt.yticks([0, 1, 2, 3, 4, 5, 6], ['Montag', 'Dienstag', 'Mittwoch', 'Donnerstag', 'Freitag', 'Samstag', 'Sonntag'], rotation=0)\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "212e4f95-bc13-45d4-8bce-65ba217361a1",
   "metadata": {},
   "source": [
    "#Kreisdiagramm Stimmung mit neutral\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Folder with JSON files\n",
    "folder_path = '/Users/hamzabartl/Documents/DataSciencePy/pythonProject/recorded-tweets'\n",
    "\n",
    "# Reading all JSON files\n",
    "all_tweets = []\n",
    "file_count = 0\n",
    "max_files = 500\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.json'):\n",
    "        with open(os.path.join(folder_path, filename), 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            try:\n",
    "                json_data = json.load(file)\n",
    "                all_tweets.extend(json_data)\n",
    "                file_count += 1\n",
    "                if file_count >= max_files:\n",
    "                    break\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Fehler beim Lesen der Datei: {filename}\")\n",
    "                \n",
    "# Converting JSON data to DataFrame\n",
    "df = pd.json_normalize(all_tweets, sep='_')\n",
    "\n",
    "# Ensure the 'text' column is of type string and remove NaNs in 'text'\n",
    "df['text'] = df['text'].astype(str)\n",
    "df = df.dropna(subset=['text'])\n",
    "\n",
    "# Remove rts\n",
    "df = df[~df['text'].str.startswith('RT')]\n",
    "\n",
    "# Cleaning tweets\n",
    "def clean_tweet(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Entfernen von URLs\n",
    "    text = re.sub(r'@\\w+', '', text)     # Entfernen von @-Erwähnungen\n",
    "    text = re.sub(r'#\\w+', '', text)     # Entfernen von Hashtags\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Entfernen von überflüssigen Leerzeichen\n",
    "    return text\n",
    "\n",
    "df['cleaned_text'] = df['text'].apply(clean_tweet)\n",
    "\n",
    "def extract_hashtags(text):\n",
    "    return re.findall(r'#(\\w+)', text)\n",
    "\n",
    "df['hashtags'] = df['text'].apply(extract_hashtags)\n",
    "\n",
    "# sentiment for pos neg and neut\n",
    "def get_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 'positive'\n",
    "    elif analysis.sentiment.polarity < 0:\n",
    "        return 'negative'\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 'neutral'\n",
    "\n",
    "df['sentiment'] = df['cleaned_text'].apply(get_sentiment)\n",
    "\n",
    "def sentiment_for_hashtags(df, hashtags):\n",
    "    hashtag_sentiments = {}\n",
    "    for hashtag in hashtags:\n",
    "        df_hashtag = df[df['hashtags'].apply(lambda x: hashtag in x)]\n",
    "        if not df_hashtag.empty:\n",
    "            sentiment_counts = df_hashtag['sentiment'].value_counts()\n",
    "            hashtag_sentiments[hashtag] = sentiment_counts\n",
    "    return hashtag_sentiments\n",
    "\n",
    "hashtags_to_analyze = ['AFD']\n",
    "\n",
    "hashtag_sentiments = sentiment_for_hashtags(df, hashtags_to_analyze)\n",
    "\n",
    "for hashtag, sentiment_counts in hashtag_sentiments.items():\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=140)\n",
    "    plt.title(f'Stimmungsverteilung für #{hashtag}')\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3cb6189-186b-4068-9bbe-676b2087a68f",
   "metadata": {},
   "source": [
    "#Kreisdiagramm Stimmung ohne neutral\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Folder with JSON files\n",
    "folder_path = '/Users/hamzabartl/Documents/DataSciencePy/pythonProject/recorded-tweets'\n",
    "\n",
    "# Reading all JSON files\n",
    "all_tweets = []\n",
    "file_count = 0\n",
    "max_files = 500\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.json'):\n",
    "        with open(os.path.join(folder_path, filename), 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            try:\n",
    "                json_data = json.load(file)\n",
    "                all_tweets.extend(json_data)\n",
    "                file_count += 1\n",
    "                if file_count >= max_files:\n",
    "                    break\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Fehler beim Lesen der Datei: {filename}\")\n",
    "                \n",
    "# Converting JSON data to DataFrame\n",
    "df = pd.json_normalize(all_tweets, sep='_')\n",
    "\n",
    "# Ensure the 'text' column is of type string and remove NaNs in 'text'\n",
    "df['text'] = df['text'].astype(str)\n",
    "df = df.dropna(subset=['text'])\n",
    "\n",
    "# Remove rts\n",
    "df = df[~df['text'].str.startswith('RT')]\n",
    "\n",
    "# Cleaning tweets\n",
    "def clean_tweet(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Entfernen von URLs\n",
    "    text = re.sub(r'@\\w+', '', text)     # Entfernen von @-Erwähnungen\n",
    "    text = re.sub(r'#\\w+', '', text)     # Entfernen von Hashtags\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Entfernen von überflüssigen Leerzeichen\n",
    "    return text\n",
    "\n",
    "df['cleaned_text'] = df['text'].apply(clean_tweet)\n",
    "\n",
    "def extract_hashtags(text):\n",
    "    return re.findall(r'#(\\w+)', text)\n",
    "\n",
    "df['hashtags'] = df['text'].apply(extract_hashtags)\n",
    "\n",
    "# sentiment for pos neg and neut\n",
    "def get_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 'positive'\n",
    "    elif analysis.sentiment.polarity < 0:\n",
    "        return 'negative'\n",
    "\n",
    "df['sentiment'] = df['cleaned_text'].apply(get_sentiment)\n",
    "\n",
    "def sentiment_for_hashtags(df, hashtags):\n",
    "    hashtag_sentiments = {}\n",
    "    for hashtag in hashtags:\n",
    "        df_hashtag = df[df['hashtags'].apply(lambda x: hashtag in x)]\n",
    "        if not df_hashtag.empty:\n",
    "            sentiment_counts = df_hashtag['sentiment'].value_counts()\n",
    "            hashtag_sentiments[hashtag] = sentiment_counts\n",
    "    return hashtag_sentiments\n",
    "\n",
    "hashtags_to_analyze = ['AFD']\n",
    "\n",
    "hashtag_sentiments = sentiment_for_hashtags(df, hashtags_to_analyze)\n",
    "\n",
    "for hashtag, sentiment_counts in hashtag_sentiments.items():\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=140)\n",
    "    plt.title(f'Stimmungsverteilung für #{hashtag}')\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc43246c-0808-4586-9c38-bd6e0c2f6cc8",
   "metadata": {},
   "source": [
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='urllib3')\n",
    "\n",
    "# Schriftart auf Standard zurücksetzen\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
    "\n",
    "# Ordner mit JSON.GZ-Dateien\n",
    "folder_path = '/Users/hamzabartl/Documents/DataSciencePy/pythonProject/7528718/german-tweet-sample-2019-04'\n",
    "\n",
    "# Alle JSON.GZ-Dateien im Ordner einlesen\n",
    "all_tweets = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.json.gz'):\n",
    "        with gzip.open(os.path.join(folder_path, filename), 'rt', encoding='utf-8', errors='ignore') as file:\n",
    "            try:\n",
    "                json_data = json.load(file)\n",
    "                all_tweets.extend(json_data)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Fehler beim Lesen der Datei: {filename}\")\n",
    "\n",
    "# Die JSON-Daten in ein DataFrame umwandeln\n",
    "df = pd.json_normalize(all_tweets, sep='_')\n",
    "\n",
    "# Sicherstellen, dass die 'text'-Spalte vom Typ string ist und NaN-Werte in 'text' entfernen\n",
    "df['text'] = df['text'].astype(str)\n",
    "df = df.dropna(subset=['text'])\n",
    "\n",
    "# Entfernen von Retweets\n",
    "df = df[~df['text'].str.startswith('RT')]\n",
    "\n",
    "# Extrahieren von Hashtags\n",
    "def extract_hashtags(text):\n",
    "    return re.findall(r'#(\\w+)', text)\n",
    "\n",
    "df['hashtags'] = df['text'].apply(extract_hashtags)\n",
    "\n",
    "# Filtern der Tweets, die den Hashtag #AfD enthalten\n",
    "df_afd = df[df['hashtags'].apply(lambda x: 'AfD' in x)]\n",
    "\n",
    "# Sicherstellen, dass die 'created_at'-Spalte als Datetime-Objekt behandelt wird\n",
    "df_afd['created_at'] = pd.to_datetime(df_afd['created_at'])\n",
    "\n",
    "# Filtern nach Tweets im April 2019\n",
    "df_afd_april_2019 = df_afd[(df_afd['created_at'] >= '2019-04-01') & (df_afd['created_at'] < '2019-05-01')]\n",
    "\n",
    "# Aggregieren der Daten nach Datum\n",
    "df_afd_april_2019['date'] = df_afd_april_2019['created_at'].dt.date\n",
    "afd_counts_april_2019 = df_afd_april_2019.groupby('date').size()\n",
    "\n",
    "# Visualisieren der aggregierten Daten in einem Liniendiagramm\n",
    "plt.figure(figsize=(12, 6))\n",
    "afd_counts_april_2019.plot(kind='line', marker='o')\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Anzahl der Tweets')\n",
    "plt.title('Anzahl der Tweets mit dem Hashtag #AfD im April 2019')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Finden des Tages mit den meisten Tweets\n",
    "max_date = afd_counts_april_2019.idxmax()\n",
    "\n",
    "# Filtern der Tweets an diesem Tag\n",
    "df_max_date = df_afd_april_2019[df_afd_april_2019['date'] == max_date]\n",
    "\n",
    "# Aggregieren der Daten nach Uhrzeit (auf Stundenebene)\n",
    "df_max_date['hour'] = df_max_date['created_at'].dt.hour\n",
    "afd_counts_hour = df_max_date.groupby('hour').size()\n",
    "\n",
    "# Visualisieren der aggregierten Daten in einem Liniendiagramm\n",
    "plt.figure(figsize=(12, 6))\n",
    "afd_counts_hour.plot(kind='line', marker='o')\n",
    "plt.xlabel('Uhrzeit')\n",
    "plt.ylabel('Anzahl der Tweets')\n",
    "plt.title(f'Anzahl der Tweets mit dem Hashtag #AfD am {max_date}')\n",
    "plt.grid(True)\n",
    "plt.xticks(range(0, 24, 2))  # Nur gerade Stunden anzeigen\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68003802-fc7d-42e2-9aa7-ab3693922232",
   "metadata": {},
   "source": [
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='urllib3')\n",
    "\n",
    "# Schriftart auf Standard zurücksetzen\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
    "\n",
    "# Ordner mit JSON.GZ-Dateien\n",
    "folder_path = '/Users/hamzabartl/Documents/DataSciencePy/pythonProject/7528718/german-tweet-sample-2019-04'\n",
    "\n",
    "# Alle JSON.GZ-Dateien im Ordner einlesen\n",
    "all_tweets = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.json.gz'):\n",
    "        with gzip.open(os.path.join(folder_path, filename), 'rt', encoding='utf-8', errors='ignore') as file:\n",
    "            try:\n",
    "                json_data = json.load(file)\n",
    "                all_tweets.extend(json_data)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Fehler beim Lesen der Datei: {filename}\")\n",
    "\n",
    "# Die JSON-Daten in ein DataFrame umwandeln\n",
    "df = pd.json_normalize(all_tweets, sep='_')\n",
    "\n",
    "# Sicherstellen, dass die 'text'-Spalte vom Typ string ist und NaN-Werte in 'text' entfernen\n",
    "df['text'] = df['text'].astype(str)\n",
    "df = df.dropna(subset=['text'])\n",
    "\n",
    "# Entfernen von Retweets\n",
    "df = df[~df['text'].str.startswith('RT')]\n",
    "\n",
    "# Extrahieren von Hashtags\n",
    "def extract_hashtags(text):\n",
    "    return re.findall(r'#(\\w+)', text)\n",
    "\n",
    "df['hashtags'] = df['text'].apply(extract_hashtags)\n",
    "\n",
    "# Filtern der Tweets, die den Hashtag #AfD enthalten\n",
    "df_afd = df[df['hashtags'].apply(lambda x: 'AfD' in x)]\n",
    "\n",
    "# Sicherstellen, dass die 'created_at'-Spalte als Datetime-Objekt behandelt wird\n",
    "df_afd['created_at'] = pd.to_datetime(df_afd['created_at'])\n",
    "\n",
    "# Filtern nach Tweets im April 2019\n",
    "df_afd_april_2019 = df_afd[(df_afd['created_at'] >= '2019-04-01') & (df_afd['created_at'] < '2019-05-01')]\n",
    "\n",
    "# Aggregieren der Daten nach Datum\n",
    "df_afd_april_2019['date'] = df_afd_april_2019['created_at'].dt.date\n",
    "afd_counts_april_2019 = df_afd_april_2019.groupby('date').size()\n",
    "\n",
    "# Visualisieren der aggregierten Daten in einem Liniendiagramm\n",
    "plt.figure(figsize=(12, 6))\n",
    "afd_counts_april_2019.plot(kind='line', marker='o')\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Anzahl der Tweets')\n",
    "plt.title('Anzahl der Tweets mit dem Hashtag #AfD im April 2019')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Finden des Tages mit den meisten Tweets\n",
    "max_date = afd_counts_april_2019.idxmax()\n",
    "\n",
    "# Filtern der Tweets an diesem Tag\n",
    "df_max_date = df_afd_april_2019[df_afd_april_2019['date'] == max_date]\n",
    "\n",
    "# Aggregieren der Daten nach Uhrzeit (auf Stundenebene)\n",
    "df_max_date['hour'] = df_max_date['created_at'].dt.hour\n",
    "afd_counts_hour = df_max_date.groupby('hour').size()\n",
    "\n",
    "# Visualisieren der aggregierten Daten in einem Liniendiagramm\n",
    "plt.figure(figsize=(12, 6))\n",
    "afd_counts_hour.plot(kind='line', marker='o')\n",
    "plt.xlabel('Uhrzeit')\n",
    "plt.ylabel('Anzahl der Tweets')\n",
    "plt.title(f'Anzahl der Tweets mit dem Hashtag #AfD am {max_date}')\n",
    "plt.grid(True)\n",
    "plt.xticks(range(0, 24, 2))  # Nur gerade Stunden anzeigen\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aabb6df-c36c-4fb6-8889-8f916bfb1bb1",
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ordner mit JSON-Dateien\n",
    "folder_path = '/Users/hamzabartl/Documents/DataSciencePy/pythonProject/recorded-tweets'\n",
    "\n",
    "# Alle JSON-Dateien im Ordner einlesen\n",
    "all_tweets = []\n",
    "file_count = 0\n",
    "max_files = 600\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.json'):\n",
    "        with open(os.path.join(folder_path, filename), 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            try:\n",
    "                json_data = json.load(file)\n",
    "                all_tweets.extend(json_data)\n",
    "                file_count += 1\n",
    "                if file_count >= max_files:\n",
    "                    break\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Fehler beim Lesen der Datei: {filename}\")\n",
    "\n",
    "# Die JSON-Daten in ein DataFrame umwandeln\n",
    "df = pd.json_normalize(all_tweets, sep='_')\n",
    "\n",
    "# Sicherstellen, dass die 'text'-Spalte vom Typ string ist und NaN-Werte in 'text' entfernen\n",
    "df['text'] = df['text'].astype(str)\n",
    "df = df.dropna(subset=['text'])\n",
    "\n",
    "# Entfernen von Retweets\n",
    "df = df[~df['text'].str.startswith('RT')]\n",
    "\n",
    "# Extrahieren von Hashtags\n",
    "def extract_hashtags(text):\n",
    "    return re.findall(r'#(\\w+)', text)\n",
    "\n",
    "df['hashtags'] = df['text'].apply(extract_hashtags)\n",
    "\n",
    "# Zählen der Häufigkeit der Hashtags\n",
    "hashtags = Counter([hashtag for hashtags in df['hashtags'] for hashtag in hashtags])\n",
    "\n",
    "# Die 10 häufigsten Hashtags visualisieren\n",
    "common_hashtags = hashtags.most_common(10)\n",
    "labels, values = zip(*common_hashtags)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(labels, values)\n",
    "plt.xlabel('Hashtags')\n",
    "plt.ylabel('Häufigkeit')\n",
    "plt.title('Top 10 Hashtags')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d43a4d4c-4539-4153-9de4-1c6f2c313b60",
   "metadata": {},
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "archive_url = \"https://web.archive.org/web/20190507094611/https://twitter.com/AfD\"\n",
    "\n",
    "response = requests.get(archive_url)\n",
    "response.raise_for_status()  # Überprüfen, ob die Anfrage erfolgreich war\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "tweets = soup.find_all('div', {'class': 'tweet'})\n",
    "\n",
    "tweets_list = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweet_id = tweet['data-item-id']\n",
    "    tweet_text = tweet.find('p', {'class': 'tweet-text'}).text if tweet.find('p', {'class': 'tweet-text'}) else ''\n",
    "    tweet_date = tweet.find('a', {'class': 'tweet-timestamp'})['title'] if tweet.find('a', {'class': 'tweet-timestamp'}) else ''\n",
    "    username = tweet['data-screen-name'] if 'data-screen-name' in tweet.attrs else ''\n",
    "    tweets_list.append([tweet_date, tweet_id, tweet_text, username])\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['date', 'id', 'content', 'username'])\n",
    "print(tweets_df.head())\n",
    "tweets_df.to_csv(\"archived_tweets_AfD_May4_2019.csv\", index=False)\n"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
